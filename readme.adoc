= Neo4j-Spark-Connector based on Neo4j 3.0's Bolt protocol
:repo: http://github.com/jexp/neo4j-spark-connector

These are the beginnings / experiments of a Neo4j-Spark-Connector using the new binary protocol for Neo4j, Bolt.

Find http://alpha.neohq.net[more information] about the Bolt protocol, available drivers and documentation.

[NOTE]
Please note that I still know very little about Apache Spark and might have done really dumb things.
Please let me know by {repo}/issues[creating an issue] or even better {repo}/pulls[submitting a pull request] to this repo.

== License

This neo4j-spark-connector is Apache 2 Licensed

== Building

Build `target/neo4j-spark-connector-1.0-SNAPSHOT_2.10-jar-with-dependencies.jar` for Scala 2.10
----
mvn clean compile assembly:single -DskipTests -Pscala_2.10
----

Build `target/neo4j-spark-connector-1.0-SNAPSHOT_2.11-jar-with-dependencies.jar` for Scala 2.11
----
mvn clean install assembly:single
----

== Config

You provide the `neo4j.bolt.url` in your `SparkConf` pointing e.g. to `bolt://localhost`.

== RDD's

There are a few different RDD's all named `CypherXxxRDD`

* `CypherTupleRDD` returns a Seq[(String,AnyRef)] per row
* `CypherRowRDD` returns a spark-sql Row per row

== DataFrames

* `CypherDataFrame`, a SparkSQL `DataFrame` that you construct either with explicit type information about result names and types
* or inferred from the first result-row

== GraphX - Neo4jGraph

* `Neo4jGraph` has methods to load and save a GraphX graph
* `Neo4jGraph.execute` runs a Cypher statement and returns a `CypherResult` with the `keys` and an `rows` Iterator of `Array[Any]`

* `Neo4jGraph.loadGraph(sc, label,rel-types,label2)` loads a graph via the relationships between those labeled nodes
* `Neo4jGraph.saveGraph(g, nodeProp, relProp)` saves a graph object to Neo4j by updating the given node- and relationship-properties
* `Neo4jGraph.loadGraphFromNodePairs(sc,stmt,params)` loads a graph from pairs of node-id's
* `Neo4jGraph.loadGraphFromRels(sc,stmt,params)` loads a graph from pairs of start- and end-node-id's and and additional value per relationship
* `Neo4jGraph.loadGraph(sc, (stmt,params), (stmt,params))` loads a graph with two dedicated statements first for nodes, second for relationships

== Example Usage

=== RDD & DataFrame

.bin/spark-shell --jars neo4j-spark-connector-1.0-SNAPSHOT_2.10-jar-with-dependencies.jar
[source,scala]
----
import org.neo4j.spark._
import org.apache.spark.sql.types._

CypherTupleRDD(sc,"cypher runtime=compiled MATCH (n) return id(n)",Seq.empty).count()
// res46: Long = 1000177

CypherRowRDD(sc,"cypher runtime=compiled MATCH (n) return id(n)",Seq.empty).count()
// res47: Long = 1000177

CypherDataFrame(sqlContext, "cypher runtime=compiled MATCH (n) return id(n) as id",Seq.empty,schema = ("id",LongType))
// res0: org.apache.spark.sql.DataFrame = [id: bigint]

res0.count()
// res1: Long = 1000177
----

=== Neo4jGraph Operations

.bin/spark-shell --jars neo4j-spark-connector-1.0-SNAPSHOT_2.10-jar-with-dependencies.jar
[source,scala]
----
import org.neo4j.spark._

val g = Neo4jGraph.loadGraph(sc, "Person", Seq("KNOWS"), "Person")
// g: org.apache.spark.graphx.Graph[Any,Int] = org.apache.spark.graphx.impl.GraphImpl@574985d8
g.vertices.count()
// res0: Long = 999937
g.edges.count()
// res1: Long = 999906
import org.apache.spark.graphx._
import org.apache.spark.graphx.lib._

val g2 = PageRank.run(g, 5)

val v = g2.vertices.take(5)
// v: Array[(org.apache.spark.graphx.VertexId, Double)] = Array((185012,0.15), (612052,1.0153273593749998), (354796,0.15), (182316,0.15), (199516,0.38587499999999997))

Neo4jGraph.saveGraph(sc, g2, "rank")
// res2: (Long, Long) = (999937,0)                                                 
----

== Driver

The project uses the http://github.com/neo4j/neo4j-java-driver[java driver] for Neo4j's Bolt protocol.
You add it via the `org.neo4j.driver:neo4j-java-driver:1.0.0-M04` dependency.

== Testing

Testing is done using `neo4j-harness`, a http://neo4j.com/docs/stable/server-unmanaged-extensions-testing.html[test library] for starting an in-process Neo4j-Server which you can use either with a JUnit `@Rule` or directly.
I only start one server and one SparkContext per test-class to avoid the lifecycle overhead. 

[NOTE]
Please note that Neo4j running an in-process server pulls in Scala 2.11 for Cypher, so you need to run the tests with spark_2.11.
That's why I had to add two profiles for the different Scala versions.
